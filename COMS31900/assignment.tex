\documentclass[paper=a4, fontsize=12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}

% Add 'Question' prefix to section numbering
\renewcommand{\thesection}{Question \arabic{section}}
% Redefine (sub-)subsection numbering to match the assignment
\renewcommand{\thesubsection}{\arabic{section}.\roman{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\alph{subsubsection}}

\begin{document}

% Force to question 3
\setcounter{section}{2}
% Question 3
\section{Range Counting}

\subsection{1-Dimension}
\label{subsec:1drangecount}

The approach to performing 1D range counting is very similar to the 1D range
searching problem as detailed in the lecture slides, week 11. The algorithm
requires sorting the input set \(S\) into an array, thus occupying \(O(n)\)
space and taking \(O(n \log n)\) preprocessing time. This sort could be
performed using HeapSort, which is \(O(n \log n)\) - no further preprocessing
is required. Performing count queries on this structure is then a simple matter
of performing binary search twice upon the array, once to find the
successor\footnote{The successor of a value \(x\) in a set \(S\) is the
 smallest \(x_s \in S\) such that \(x_s \geq x\). Similarly, the predecessor
 of \(x\) in \(S\) is the smallest \(x_p \in S\) such that \(x_p \geq x\).} of
\(x_1\), and again to find the predecessor\footnotemark[\value{footnote}] of
\(x_2\). Binary search is in \(O(\log n)\), thus performing it twice gives us
\(O(2 \log n)\), that is equivalent to \(O(\log n)\). The final step is a
simple \(O(1)\) subtraction operation on the indices returned from binary
search, giving the count of elements in the range \(x_1 \leq x \leq x_2\). The
following pseudocode outlines this algorithm.

\begin{lstlisting}
# Receive as input a set S of n 1D points, and two bounds X1 and X2

# Sort the input set using an O(n log n) algorithm
Sorted = NlogNSort(S)

# Use binary search to find the index of the successor of X1 (O(log n))
SuccessorX1 = SuccBinarySearch(Sorted, X1)
# Use binary search again to find index of the predecessor of X2 (O(log n))
PredecessorX2 = PredBinarySearch(Sorted, X2)

# Take the difference of the two returned indices (O(1))
k = PredecessorX2 - SuccessorX1

# k should now hold the number of elements in the given range
return k
\end{lstlisting}

As previously mentioned, in the above pseudocode the sorting step could be
implemented using HeapSort. The two functions SuccBinarySearch and
PredBinarySearch find the index\footnote{Note that the definition of successor
 and predecessor have been altered slightly here, to account for the corner
 case where the value being searched for is greater than the largest element
 of \(S\). In this case, the index returned should be \(|S|\), presuming a
 0-based indexing system.} of the successor and predecessor of a given value
in the supplied sorted array, respectively. In the Python programming language,
the bisect module provides implementations of binary search that can be used to
perform these operations.

\subsubsection{Correctness}

As \(Sorted\) is a sorted array, it holds that
\begin{align*}
\forall x_i, x_{i+1} \in Sorted : x_i \leq x_{i+1}.
\intertext{SuccBinarySearch finds the value of \(i_s\), given some \(x_1\), such that}
\forall x_i \in Sorted, i \geq i_s : x_1 \leq x_i.
\intertext{Similarly, PredBinarySearch finds \(i_p\), given some \(x_2\), such that}
\forall x_i \in Sorted, i < i_p : x_i \leq x_2.
\intertext{Combining these three properties, therefore}
\forall i \in \mathbb{N}_0, i \geq i_s, i < i_p : x_1 \leq Sorted_i \leq x_2.
\intertext{\(Sorted\) is contiguous, thus we can say that}
i_p - i_s = k
\intertext{where}
k = |X|, X = \{\, x : x \in U, x_1 \leq x \leq x_2 \,\}.
\end{align*}

To summarise, as \(Sorted\) is a sorted array, we can find the smallest and
largest elements in the desired range using binary search. We can then subtract
the difference in indices of the smallest element, and the element following
the largest. This will give us the number of elements, \(k\), in the desired
query range.

\subsection{2-Dimensional}
\label{subsec:2drangecount}

Expanding the range counting problem into 2 dimensions requires building a
balanced binary tree\footnote{For example, an AVL tree.}, using one dimension
as the key. To ease explanation with regards to the 1-Dimensional case, the
\(y\) coordinate will be used to build this tree. Each node in the tree will
represent a single point in \(U\), and so will store an additional field that
is the \(x\) coordinate. Additionally, in order to meet the desired worst-case
time guarantees, each node will hold a sorted array, in the same manner as
subsection \ref{subsec:1drangecount}. The elements of this array will be the
\(y\) coordinates of all points in the subtree rooted at this node (thus also
including this node's \(y\) value).

This data structure requires \(O(n \log n)\) space. This is explained by the
lecture slides, week 11, where each level of the tree holds nodes containing
arrays that are disjoint subsets of \(U\). Each node also holds an extra
element for the associated \(x\) coordinate, taking up an additional \(O(n)\)
space. Thus, the data structure is \(O(n \log n + n)\), which is equivalent to
\(O(n \log n)\).

The preprocessing time for this data structure is also similar to the lecture
slides. The only thing not explicitly stated in the slides that is present here
is that each node also holds \(y\). These can simply be inserted whilst
creating the tree, thus not increasing the worst case preprocessing time.

Performing a range counting query on this structure requires searching the tree
for the split node. Then, the successor of \(y_1\) must be found in the tree,
by recursing down the tree from the split. At each node along the search path,
if that node's \(x\) coordinate is within the desired range, the node's \(y\)
coordinate should be checked, and the count should be incremented by \(1\). The
algorithm then checks the right child of the current node, and performs the 1D
range counting algorithm (subsection \ref{subsec:1drangecount}), adding the
returned count to the running total. This algorithm is detailed in the
following pseudocode:

\begin{lstlisting}
# Receive as input four bounds (X1,Y1) and (X2,Y2) and the preprocessed tree data structure rooted at Root

# Search the tree from root, returning the node where the searches for Y1 and Y2 would diverge - i.e. the highest node in the tree with Y1 <= Y <= Y2 (O(log n))
Split = FindSplit(Root, Y1, Y2)

# Check that the region between Y1 and Y2 actually intersects with U (O(1))
if Split is None:
  # Y1 > max(U.y) or Y2 < min(U.y)
  return 0

# Initialise the count, k, to 0 (O(1))
k = 0

# Check the x coordinate held in Split, and add this to the count if suitable (O(1))
if X1 <= Split.x <= X2:
  k = k + 1

# Search down the tree for the successor of Y1. (O(log n) iterations)
Current = Split.left
while Current is not None:
  if Y1 <= Current.y:
    # Count this node if the X coordinate is within range. (O(1))
    if X1 <= Current.x <= X2:
      k = k + 1

    # The entire right subtree must be within the Y range. (O(log n))
    k_right = RangeCount1D(Current.right.sorted, X1, X2)
    k = k + k_right

    # Recurse to the left subtree. (O(1))
    Current = Current.left
  else:
    # This node is outside the range, recurse to the right subtree. (O(1))
    Current = Current.right

# Search down the tree for the predecessor of Y2. (O(log n) iterations)
Current = Split.right
while Current is not None:
  if Current.y <= Y2:
    # Count this node if the X coordinate is within range. (O(1))
    if X1 <= Current.x <= X2:
      k = k + 1

    # The entire left subtree must be within the Y range. (O(log n))
    k_left = RangeCount1D(Current.left.sorted, X1, X2)
    k = k + k_left

    # Recurse to the right subtree. (O(1))
    Current = Current.right
  else:
    # This node is outside the range, recurse to the left subtree. (O(1))
    Current = Current.left

# k should now hold the number of elements in the given range
return k
\end{lstlisting}

In the above pseudocode, \(Root\), \(Split\), and \(Current\) are nodes,
holding properties that describe the corresponding coordinates (as \(Node.x\)
and \(Node.y\)), the left and right child nodes (as \(Node.left\) and
\(Node.right\), that are None if no child exists in that direction), and the
sorted array of all x coordinates in the subtree rooted at that node (as
\(Node.sorted\)). The algorithm RangeCount1D is that which is described in
subsection \ref{subsec:1drangecount}.

\subsubsection{Query Runtime Analysis}

As is discussed in the lecture slides, week 11, and reasserted in the above
pseudocode, findig the successor of \(y_1\) takes \(O(\log n)\) time. This is
due to the search visiting only a single node at each level of the
tree\footnote{More strictly, two nodes are visited at each level, including the
 target of each 1DRangeCount invocation.}. As the tree is a balanced
binary search tree, it has \(O(\log n)\) depth. At each level of the tree,
there is a possible invocation of \(1DRangeCount\). As shown in subsection
\ref{subsec:1drangecount}, this query is \(O(\log n)\), due to the binary
searches performed on the sorted array. The other operations performed at each
iteration of the search are all \(O(1)\). Therefore, the entire search
operation for the predecessor of \(y_1\) is \(O(\log^2 n)\). Comparing this to
the search for the predecessor of \(y_2\) reveals that the algorithm is the
same but with mirrored comparisons and recursion directions. Thus this loop
must also be \(O(\log^2 n)\), and the entire query algorithm must be in
\(O(2\log^2 n)\), which is equivalent to \(O(\log^2 n)\).

\subsubsection{Correctness}

The definition of a binary search tree states that
\begin{align*}
\forall N \in TreeNodes : N_{Left} \leq N < N_{Right}
\intertext{and (treating a node, n, as the set of nodes in the subtree rooted at n)}
\forall N \in TreeNodes, \forall L \in N_{Left} : L \leq N \\
\forall N \in TreeNodes, \forall R \in N_{Right} : N < R
\intertext{The algorithm presented here adjusts this definition slightly, using (keyed) tuples as nodes, and ordering nodes based upon the \(y\) coordinate stored in each - e.g.}
\forall N \in TreeNodes : y_{N_{Left}} \leq y_N < y_{N_{Right}}.
\end{align*}

This algorithm begins by searching for the highest node, \(Split\)\footnote{Closely related to the Lowest Common Ancestor in terms of trees, albeit in this case we have no guarantee that the values we are considering are actually nodes in the tree.}, such that
\begin{align*}
y_1 \leq y_{Split} \leq y_2
\intertext{The algorithm then searches to the left of \(Split\) to find the leftmost (or lowest) leaf, \(L\), where}
y_1 \leq L
\intertext{From the binary tree definitions, all nodes to the left of split must have \(y\) values that do not exceed \(y_2\). Thus, at each node \(Curr\) in the search path, it can be shown that there are two possibilities based upon the value of \(y_{Curr}\) for each child's subtree, i.e.}
\forall Curr \in LeftPath, \forall R \in Curr_{Right} :
\begin{cases}
y_1      \leq y_R \leq y_2 & \text{if } y_{Curr} \geq y_1 \\
y_{Curr} \leq y_R \leq y_2 & \text{if } y_{Curr}   <  y_1
\end{cases}&
\intertext{and}
\forall Curr \in LeftPath, \forall L \in Curr_{Left} :
\begin{cases}
y_L \leq y_{Curr} & \text{if } y_{Curr} \geq y_1 \\
y_L   <  y_1      & \text{if } y_{Curr}   <  y_1
\end{cases}&.
\end{align*}
Thus, at any point in in the search path, there is a single subtree that is either completely excluded from the query range (and is not considered when modifying the total) or completely included in the query range in terms of the key dimension, \(y\) (and is thus considered in terms of the \(x\) coordinates using 1DRangeCount. The opposite subtree may or may not contain points within the query range, and thus must be considered in terms of it's children - i.e. that child becomes \(Curr\), and the algorithm recurses another level down the tree. Clearly, this results in the running total being incremented for every single valid point in \(LeftPath\), apart from the point represented by \(Curr\) itself. However, the algorithm explicitly increments the running total by 1 if \(x_1 \leq x_{Curr} \leq x_2\) and \(y_1 \leq y_{Curr} \leq y_2\) for every \(Curr\).

Once the algorithm has reached the end of the left search path (i.e. the point where the uncertain subtree is in fact empty), the same arguments can be applied to the right search path, which is nothing more than the mirror image of the left case. Therefore, we have shown that both the left and right search paths have accounted for all within range points either side of the Split. This leaves only the point stored at \(Split\) neglected, which is covered at the beginning of the algorithm with an explicit check of \(x_{Split}\). With the assumption that the 1DRangeCount algorithm has proven correctness (shown in subsection \ref{subsec:1drangecount}), this algorithm must return the count of elements in the desired range.

\subsection{d-Dimensions}

Generalising the 2-dimensional range counting problem into d-dimensions is straightforward, as it involves expanding the tree structure used to \(d-1\) nested trees, where the innermost trees hold the sorted arrays of the final dimension. In other words, to create a data structure for querying in 3 dimensions (\(d = 3\), with dimensions \(\{\,x,y,z\,\}\)), a balanced tree keyed by the third dimension (\(z\)) should be built. Each node in this tree should contain a 2D data structure (as described in subsection \ref{subsec:2drangecount}) for all points in the subtree rooted at that node. In terms of space, the 2D range counting structure is \(O(n \log n)\). As the outer tree is balanced, it has \(\log n\) levels, and each level contains up to \(n\) points, partitioned between the nodes on that level. Thus, the overall space is \(O(n \log^2 n)\). Expanding this to an extra dimension simply nests another tree over the existing trees, thus increasing the power on the log, giving a space requirement of \(O(n \log^{d-1} n)\).

The preprocessing time needed to create this data structure is also \(O(n \log^{d-1} n)\). This follows from the 2D situation in a similar manner to the space requirement, where each dimension requires a balanced tree, except for the first, which is comprised of sorted arrays.

Performing a range counting query on this data structure is very similar to the 2-dimensional case. The query can be performed in \(O(\log^d n)\) time, by searching down the outer-most tree from the split point, as in the 2D case. Each node considered along the search path will contain a nested structure for \(d - 1\) dimensions, invoking the algorithm on this subset of the input dimensions. Eventually the algorithm will reach the base case of \(d = 2\), performing the algorithm described in subsection \ref{subsec:2drangecount}.

% Question 4
\section{Bin Packing Approximations}

\subsection{6 Approximation}

A 3 approximation for this bin packing problem can be found by making use of
the FirstFitDecreasing algorithm, with the set of all gems partitioned into two
subsets. One such subset should hold only a single type of gem, whilst the
other subset should hold the remaining two types of gem. The following proofs
do not consider the effect of choosing different gem types to partition around;
in my algorithm I will simply partition the n input gems into a subset AB,
containing all gems of type A and B, and a subset C, containing all gems of
type C. The following pseudocode makes use of a FirstFitDecreasing function,
that implements the algorithm as discussed in the lecture slides. This function
takes as input a list of gems and outputs the number of sacks required.

\begin{lstlisting}
# Receive as input a list N of n gems, i.e. n = |N|

AB = []
C  = []

# Partition the gems, O(n)
for gem in N:
  if gem.type == C:
    C.append(gem)
  else:
    AB.append(gem)

s = 0

# As only two types of gem, cannot mix more than two types in a sack. O(n^2)
s = FirstFitDecreasing(AB)

# Repeat for type C, again O(n^2)
s = s + FirstFitDecreasing(C)

# s should now hold the count of bins required
return s
\end{lstlisting}

\subsubsection{Runtime Analysis}
From the lecture slides, week 13, we have that the FirstFitDecreasing algorithm
is \(O(n^2)\), from the sorting operation that needs to be performed before
assigning items to bins. As the annotated pseudocode describes, we perform an
\(O(n)\) preprocessing step whereby the gems are split into two subsets. This
is a simple loop over all elements in N; this loop is executed n times,
performing \(O(1)\) operations in the body of the loop. The algorithm then runs
the FirstFitDecreasing algorithm twice, giving a worst case runtime of \(O(2n^2
+ n)\), which is equivalent to \(O(n^2)\). By definition, this is polynomial
time.

\subsubsection{Correctness}
From the lecture slides, week 13, the FirstFitDecreasing algorithm produces
valid solutions; the algorithm operates by finding the first bin that satisfies
the equation \(t + w \leq c\), where t is the total weight of items already in
the bin, w is the weight of the item currently being considered, and c is the
capacity of each bin (c = 1.0, in this case).

The algorithm must therefore produce valid solutions for both subsets of N (AB
and C), at least in terms of sack capacity. However, the problem also specifies
a restriction on the mixing of gem types; no sack may contain a gem of type A,
a gem of type B, and a gem of type C. This algorithm avoids violating this
condition by partitioning the gems such that the FirstFitDecreasing algorithm
is never supplied with gems of all three types at once, so it cannot produce an
invalid packing. The algorithm combines the two packings (of AB and of C), by
simply concatenating the two; or in this case, by summing the outputs of
FirstFitDecreasing. In other words, the number of sacks needed for all gems of
type A and B is found, then the number for type C is found, and the final
solution keeps type C completely separated from the others.

\subsubsection{Approximation Factor}
From the lecture slides, week 13, we are shown that the FirstFitDecreasing
algorithm produces a \(\frac{3}{2}\) approximation. \((Opt \leq s \leq
\frac{3}{2} Opt)\). Consider each subset individually; denoting the optimal
solution for the AB subset as \(Opt_{AB}\), and similarly \(Opt_C\) for the C
subset. Running FFD on these subsets will produce packings that satisfy:

\begin{alignat*}{2}
Opt_{AB}         &\leq        &s_{AB} \quad &\leq \quad \frac{3}{2} Opt_{AB}
\intertext{and}
Opt_C            &\leq        &s_C \quad    &\leq \frac{3}{2} Opt_C
\intertext{that when combined give an inequality for the overall approximation}
Opt_{AB} + Opt_C &\leq s_{AB} &+ s_C        &\leq \frac{3}{2} Opt_{AB} + \frac{3}{2} Opt_C \\
Opt_{AB} + Opt_C &\leq        &s \quad      &\leq \frac{3}{2} Opt_{AB} + \frac{3}{2} Opt_C
\end{alignat*}

In order to demonstrate the bounds of the approximation in terms of the global
optimum, \(Opt\), i.e. the optimum when considering all three types
simultaneously, it is necessary to show the relationship between the two local
optimums, \(Opt_{AB}\) and \(Opt_C\), and \(Opt\). It must be the case that
\(Opt_C \leq Opt\). This can be explained by considering that C is a subset of N,
thus by definition C must have at most n elements. Therefore, in the most
extreme case the entirety of N will in fact be gems of type C - thus the local
optimum packing for C will be equal to the global optimum for N, \(Opt\). The
only other possibility is that the subset C has fewer elements than N. Clearly
in this case, \(Opt_C \leq Opt\), as removing items from the input cannot
possibly increase the required number of bins, as all gems have positive
weights.

The same argument can be made for the subset AB, as there are no restrictions
on sacks containing only two types of gem other than the standard capacity
limit. Thus it follows that \(Opt_{AB} \leq Opt\). This can be used to express
the upper bounds of the above approximation inequality in terms of the global
optimum, like so:

\begin{alignat*}{2}
Opt_{AB} + Opt_C &\leq \quad &s \quad &\leq \frac{3}{2} Opt_{AB} + \frac{3}{2} Opt_C \\
Opt_{AB} + Opt_C &\leq \quad &s \quad &\leq \frac{3}{2} Opt + \frac{3}{2} Opt \\
Opt_{AB} + Opt_C &\leq \quad &s \quad &\leq 2 ( \frac{3}{2} Opt ) \\
Opt_{AB} + Opt_C &\leq \quad &s \quad &\leq 3 Opt
\end{alignat*}

In a similar manner, the lower bound can be simplified by observing that in the
best case, all elements of N are grouped into only one of \(Opt_{AB}\) or
\(Opt_C\). In this situation, either \(s_{AB} = 0\) or \(s_{C} = 0\) will be
true, thus \(s = s_x\). Going back to the definition of FirstFitDecreasing,
this means that the lower bound of the inequality must be \(Opt\). In any other
situation, the lower bound must be greater than or equal to \(Opt\) - this is
clear from the definition of Opt as the most efficient packing possible - it
cannot be beaten. Thus, we can simplify the entire inequality as follows:

\begin{alignat*}{2}
Opt_{AB} + Opt_C &\leq \quad &s \quad &\leq 3 Opt \\
Opt              &\leq \quad &s \quad &\leq 3 Opt
\intertext{\centering Which clearly satisfies:}
Opt              &\leq \quad &s \quad &\leq 6 Opt
\end{alignat*}

\subsection{Tighter Approximation}

A 2 approximation for the gem packing problem can be achieved by modifying the
NextFit algorithm - albeit with a constant factor on the worst case
performance, giving the inequality:

\begin{displaymath}
Opt \leq s \leq 2 Opt + 1
\end{displaymath}

This is achieved by sorting the input list, N, into type order, such that all
gems of type A are followed by those of type B, and then by type C. This sorted
input list is then fed through a modified version of the NextFit algorithm,
whereby an extra check is added to ensure that a bin holding both A and B gems
will not be considered suitable for a gem of type C. (In this case, the current
bin will move to the next, as if the capacity limit for the current bin had
been exceeded by this differently typed gem.) In the following subsections it
will be shown that this condition can only be triggered under a very specific
set of conditions, and will only result in one extra bin being added to the
output. The pseudocode below details this approach.

\begin{lstlisting}
# Receive as input a list N of n gems, i.e. n = |N|

A = []
B = []
C = []

# Partition the gems, O(n)
for gem in N:
  if gem.type == A:
    A.append(gem)
  elif gem.type == B:
    B.append(gem)
  elif gem.type == C:
    C.append(gem)

# Current sack number
s = 0
# Current sack weight total
total = 0.0
# Does current sack contain an A type gem?
hasA = False

# Concatenate lists A, B, and C (O(1))
L = A + B + C

# Loop through all gems in L, adding to the current bin until it is no longer
# possible, then moving to the next bin. (O(n))
for gem in L:
  hasA = hasA or (gem.type == A)
  total = total + gem.weight
  if (total > 1.0) or (hasA and gem.type == C):
    # The current bin is unsuitable for this gem, move to the next
    s = s + 1
    total = gem.weight
    hasA = (gem.type == A)

# Need to check if the current bin has been used.
if total > 0.0:
  s = s + 1

# s should now hold the count of bins required
return s
\end{lstlisting}

Note that this implementation will work with any chosen permutation of list
concatenation in L - e.g. \(L = C + B + A\).

\subsubsection{Runtime Analysis}
From the lecture slides, week 13, we have that the NextFit algorithm is
\(O(n)\). As the annotated pseudocode describes, we perform an \(O(n)\)
preprocessing step whereby the gems are sorted into three subsets based upon
their type. This is a simple loop over all elements in N; this loop is executed
n times, performing \(O(1)\) operations in the body of the loop. The algorithm
then runs the modified NextFit algorithm. As demonstrated in the pseduocode,
this modified NextFit loop continues to be \(O(n)\) - the only modifications
being the addition of some constant time operations to the loop body, that runs
n times. Overall this gives a worst case runtime of \(O(2n)\), which is
equivalent to \(O(n)\). By definition, this is polynomial time.

\subsubsection{Correctness}
The core bin packing loop of the algorithm shown here is very similar to the
standard NextFit algorithm described in the lecture slides, week 13. The
standard NextFit algorithm produces valid solutions; the algorithm checks that
adding an element to the current bin will not exceed it's capacity (1.0 for the
given problem). This check is still present in the modified algorithm, such
that the restriction on total bin weight will never be exceeded.

The second restriction, whereby the mixing of all three gem types may not share
a single sack, is guaranteed by a combination of the preprocessing sort by
type, and the check against adding a C type gem to a bin that holds an A type
gem. The input list, L, is iterated over in order of type. Thus once we begin
considering C type gems, there will be no more A or B type gems to be
processed. Therefore, it is sufficient to check that when adding a C type gem
that the current bin does not contain an A type gem. In this case, the same
action is taken as is used to handle an over-capacity bin - i.e. the current
bin is moved to a new bin, and the current gem is inserted into this new
bin. This ensures that all three types are never placed into the same sack.

\subsubsection{Approximation Factor}
In most cases this modified algorithm will follow the performance of the
original NextFit algorithm. From the lecture slides, week 13, we have that
NextFit finds approximations for s in the range \(Opt \leq s \leq 2 Opt\). The
only time that the algorithm presented here performs packing differently from
the original is when a C type gem is encountered and the current bin contains
at least one A type gem. Due to the sort operation performed as a preprocessing
step, there is only one situation where an A type gem may exist in the current
bucket whilst considering a C type gem. This will be the case if, and only if:

\begin{displaymath}
\exists a \in A, \exists c \in C \text{ \quad such that \quad } a.weight + \sum\nolimits_{b \in B} b.weight + c.weight \leq 1.0
\end{displaymath}

In this special case, the current bin is finished early, and a new bin is
considered. This shifting operation wastes less than one sack's worth of
capacity, thus adding a single sack onto the found packing in the worst
case. Therefore we find the approximation to be described by the following
inequality:

\begin{displaymath}
Opt \leq s \leq 2 Opt + 1
\end{displaymath}

The lower bound here can be achieved in the case where there is only a single input gem, among others. As per the definition of \(Opt\), clearly the algorithm cannot find a packing better than this limit.

\end{document}
