OPENMP Coursework Notes
=======================

1) Compile using tau and run to get profiler results.

Total runtime unoptimised: 4:03.525
Slowest function is collision()

2) Modify collision() to stop recomputing curr_cell

New time is 3:53.237

3) Collision again, directional velocity

This was stupid, slower. 3:54.088

4) See the effect when switching to -O3

Much much faster, 1:51.143

5) Try naiive parallel for in collision

Down to 38.661s

6) Try adding omp_set_nested(1)

At 38.586 - negligible

7) Attempt parallelisation of accelerate_flow

Slower! 39.146

8) That was silly, par av_velocity with a reduction after checking next in prof

Down to 27.279!

9) Next to par is propagate

DOWN TO 9.756s MO-FOS.

10) Par reduction on total_density

Negligible effect, now at 9.760s

11) Try total_density with fewer threads.

Same as above. 9.771s

12) Revert to serial total_density

Back to 9.754s

13) Parallel in rebound

Slightly faster @ 8.952

14) Re-compile with tau and check new timings

Highest times on parallelfor body <308,387> - collision().
Followed by barrier entry/exit on <308,387> and <266,283> and others.

15) Try manually storing w1|w2 * local_density

Slower - 9.138s

16) Revert 15. Try collapsing nested collision loops manually.

Slower - 9.912s

17) Stop calculating rows/cols in collapsed loop.

Now at 9.054s - assuming no change to original nested loops.

18) Turn on -ffast-math, #YOLO

MAXIMUM SPEEEEED. 5.081s

19) Profile. Majority of time is now spent on barrier entry/exit. Can we shorten this?

Otherwise: 1741 msec for collision body, 1140 msec for propagate

20) Try storing current cell in propagate.

Slower, 5.134s

20.1) Try to figure out a better way to access adjacent cells. Fail.

21) Try switching propagate to guided scheduling.

Much slower! 7.386s!

22) Revert and try the same on collision.

Faster than propagate but still worse. Longer function has more room for variation? 6.789s

22.1) Reverted back to 5.090s, checked verification.

23) Checked 1024x1024 example.

Runtime 125.2s. Verification passed.

24) Try manually flattening nested loops in av_velocity.

No change! 5.056s

25) Try adding march flag

Perhaps a very small change? 5.045s

26) Try flattening propagate?

Need to know ii and jj in order to wrap around edges... Better way to calculate?

27) Use curr_cell in rebound.

Possible minimal speedup. Best is 5.026s

28) Add a compile time switch to use single precision floats.

Compiles fine in normal mode, time still 5.033s.
Compiles fine in single precision mode, time reduced to 4.469s. Verification passes!
Large input in single mode = 72.648s! Passing!

