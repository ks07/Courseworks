Inserting into the hashtable is much faster than the linked list. For example, using my second hash function gives an insertion time of approx. 4.74 seconds for the list, compared to a mere 0.02 seconds for the hashtable, which is a speedup of over 200x. Using significantly larger texts, such as a copy of "Les Miserables", greatly increases this speedup, with times of ~160s for the list and ~0.3s for the table.

To select a hash function, I first started by coding two very simple algorithms. The first was a simple sum of all characters, while the second was an improved variation, where I grouped the characters into integers before summing them. I also implemented 3 hash functions that I found available online, that were said to show good characteristics. Looking at the comparison table* I produced using the sample test, I decided to use the FNV-1a algorithm. On average, it required the fewest comparisons for each lookup, it was joint best for worst comparisons (in terms of overflow), and it was the second best algorithm in terms of bucket usage. You will notice all the external algorithms gave similar times, while my two simple hashes were slower.** You will also notice that, unfortunately, all of the empty bucket percentages are quite poor, considering that in all cases the load factor was 0.71. Improvements to this could be made by trialling more hash functions, or tweaking both the number of buckets, and the rate at which this number is increased.

For the above tests, I chose a (possibly less than perfect) set of defaults for the hashtable. The values I chose were 1000, 1.33, and 0.8, which represent the initial number of buckets, the resize factor, and the limit at which the load factor is great enough to resize the table. To try and improve the performance and efficiency of my hashtable, I tried a selection of different parameters.*** Through these tests, I determined that, in general, the best results are found when resizing little and often. This is problematic however, as every resize means re-hashing and re-arranging all keys in the table. I also found that tuning the parameters to the input, and thus not requiring any resizes, improved the efficiency. Obviously, these values would not work so well on other inputs, so these tests are largely irrelevant, although interestingly the case for 9213 inital buckets is marginally worse than it's nearest prime number, 9221. This suggests that using prime numbers for the modulus does indeed help to achieve an even spread and fewer collisions. Finally, I noticed that fewer initial bucket generally fared worse than larger bucket counts. This could be an issue when dealing with extremely varied inputs, as a larger starting count will be detrimental when the program is dealing with significantly fewer keys. All the above considered, I decided to use 2099 initial buckets, with a resize factor of 1.5 and a resize limit of 1.0.

Lookups using the hashtable are also much less expensive than the linked list. As shown in the table comparing hashing algorithms, in all of the example cases, the number of comparisons required to lookup the key in the linked list is enormous in comparison to even the worst case of 5 comparisons when using the hashtable and FNV-1a. Of course, there are a very small number of cases where the linked list approach might require less comparisons than the hashtable, but chances of this being problematic are negligible. My program has room for improvement when considering the linked list and lookups. Sorting the list would allow fewer comparisons for words which are not present, at the cost of even greater insertion time. This improvement could be further developed by adapting the linked list to become a skip list, which would allow a binary search to take place on it's contents, without dramatic changes to the storage structure.

To conclude, I have seen that hashtables are incredibly efficient in terms of time, which is highlighted by the exact opposite nature of the linked list. On the downside, I have seen that finding a hashing algorithm for unknown data is incredibly difficult, and can result in a fair amount of wasted memory usage when many buckets remain unfilled. With modern personal computers, this is not an overwhelming problem, as RAM has decreased in price. However, perhaps when running on an embedded system, the linked list may be preferable, if time spent on lookups is not critical, while memory space is severley restricted.

* See Table 1, comparison.txt
** Note that all times are taken with GCC's optimisations (-O2) enabled, which is likely to be the case in real world scenarios. Without optimisations, the times are more varied, although the first two are still decidedly slower.
*** See Table 2, comparison.txt
